
# ğŸ“š Semantic RAG Chatbot with Multi-Turn & Cohere

This project is a **native language QA chatbot** that performs **retrieval-augmented generation (RAG)** on local documents.  
It uses **LlamaIndex** + **Cohere** to:
- Build a vector index from PDFs, DOCX, and TXT files (with OCR for scanned PDFs)
- Query all indexed documents every time â€” guaranteeing fresh answers
- Optionally support **multi-turn conversation**, so follow-up questions have context
- Display retrieved chunks, metadata, summaries, and named entities

---

## ğŸš€ Features

âœ… Hybrid RAG (summaries, entities, full text)  
âœ… Page-level OCR for scanned PDFs  
âœ… Cross-document search â€” answers always come from **all indexed files**  
âœ… Multi-turn conversation tracking (Streamlit session state)  
âœ… Interactive Streamlit web app

---

## ğŸ—‚ï¸ Project Structure

```
.
â”œâ”€â”€ cohere_client.py      # Wraps Cohere API calls (embedding, summary, extraction, answer gen)
â”œâ”€â”€ ingestion.py          # Handles OCR, text extraction, metadata enrichment, index build
â”œâ”€â”€ query_engine.py       # Runs queries against the vector index + calls Cohere to compose answers
â”œâ”€â”€ main.py               # Streamlit front-end
â”œâ”€â”€ test_script.py        # Simple tests for single-turn and multi-turn queries
â”œâ”€â”€ config.py             # Loads COHERE_API_KEY
â”œâ”€â”€ requirements.txt      # All dependencies
â””â”€â”€ storage/              # Saved vector index
```

---

## âš™ï¸ Requirements

- Python 3.9+
- [Cohere API Key](https://dashboard.cohere.com/api-keys)

Install all dependencies:

```bash
pip install -r requirements.txt
```

---

## ğŸ”‘ Setup

1ï¸âƒ£ Get your Cohere API key and **add it to your environment**:

```bash
Replace your key in config.py

Here: COHERE_API_KEY = os.getenv("COHERE_API_KEY", "your_cohere_api_key")


---

## ğŸ“¥ Index your documents

1. Place your **PDF**, **DOCX**, or **TXT** files in a folder.
2. Run the Streamlit app:

```bash
streamlit run main.py
```

3. Enter your folder path and click **Build Index**.

---

## ğŸ’¬ Ask questions

- Once the index is built, type your question and click **Get Answer**.
- The app:
  - Retrieves relevant context chunks across **all docs**
  - Composes an answer using Cohereâ€™s LLM with multi-turn memory
  - Displays retrieved metadata for full transparency

---

## âœ… Example usage

Try:

```
Q: Who is Charbel?
Q: What is The Phoenix Alliance?
Q: What is his phone number?
```

Answers will combine fresh retrieval and your conversation history.

---

## ğŸ§© Tests

Run the included test script to verify:

```bash
python test_script.py
```

---

## ğŸ—‚ï¸ Tech Stack

- **Python** â€” main glue
- **Streamlit** â€” web UI
- **LlamaIndex** â€” vector store & retrieval
- **Cohere** â€” embeddings, summarization, entity extraction, final LLM answers
- **PyMuPDF + pytesseract** â€” robust PDF text extraction with OCR fallback

---

## âš ï¸ Notes

- Keep your **Cohere usage limits** in mind (10 calls/minute for free tier), that's why we added sleep(6) and the "build index" process takes around 2 minutes.
- The vector index is stored in `./storage` â€” rebuild if your docs change.

---

## ğŸ“„ License

This project is provided as a technical demo â€” adjust and adapt for your needs!

---

**Made with â¤ï¸ and Cohere**
